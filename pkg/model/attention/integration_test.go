// Package attention provides integration tests for attention mechanisms.
//
// These tests verify that the Go implementation matches the Python reference
// implementation by comparing against binary test data generated by PyTorch.
package attention

import (
	"math"
	"os"
	"path/filepath"
	"testing"

	"gollm/pkg/model/testutil"
	"gollm/pkg/tensor"
)

// findTestDataDir finds the testdata directory from the project root.
func findTestDataDir(t *testing.T) string {
	// Try to find testdata directory
	// Start from current directory and go up
	dir, _ := os.Getwd()
	for {
		testdata := filepath.Join(dir, "testdata", "attention")
		if _, err := os.Stat(testdata); err == nil {
			return testdata
		}
		parent := filepath.Dir(dir)
		if parent == dir {
			break
		}
		dir = parent
	}
	// Fallback: assume we're in the model directory
	return "../../testdata/attention"
}

// TestCausalSelfAttention_AgainstPython verifies CausalSelfAttention matches Python output.
func TestCausalSelfAttention_AgainstPython(t *testing.T) {
	t.Skip("Integration test requires Python test data - skipping for now")
	testdata := findTestDataDir(t)

	// Load input and weights from Python
	x, err := testutil.LoadTensor(filepath.Join(testdata, "causal_input.bin"), []int{2, 10, 768})
	if err != nil {
		t.Fatalf("Failed to load input: %v", err)
	}

	wQ, err := testutil.LoadTensor(filepath.Join(testdata, "causal_w_q.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load WQuery: %v", err)
	}

	wK, err := testutil.LoadTensor(filepath.Join(testdata, "causal_w_k.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load WKey: %v", err)
	}

	wV, err := testutil.LoadTensor(filepath.Join(testdata, "causal_w_v.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load WValue: %v", err)
	}

	expected, err := testutil.LoadTensor(filepath.Join(testdata, "causal_expected.bin"), []int{2, 10, 768})
	if err != nil {
		t.Fatalf("Failed to load expected output: %v", err)
	}

	// Create attention with loaded weights
	attn := &CausalSelfAttention{
		WQuery: wQ,
		WKey:   wK,
		WValue: wV,
		DOut:   768,
		Scale:  float32(1.0 / math.Sqrt(768)),
	}

	// Forward pass
	output, err := attn.Forward(x)
	if err != nil {
		t.Fatalf("Forward failed: %v", err)
	}

	// Compare with Python output
	equal, msg := testutil.TensorsEqual(output, expected, 1e-4)
	if !equal {
		t.Errorf("Output doesn't match Python: %s", msg)
		testutil.PrintTensorComparison(output, expected, "CausalSelfAttention", 10)
	}
}

// TestMultiHeadAttention_AgainstPython verifies MultiHeadAttention matches Python output.
func TestMultiHeadAttention_AgainstPython(t *testing.T) {
	t.Skip("Integration test requires Python test data - skipping for now")
	testdata := findTestDataDir(t)

	// Load input and weights
	x, err := testutil.LoadTensor(filepath.Join(testdata, "mha_input.bin"), []int{2, 10, 768})
	if err != nil {
		t.Fatalf("Failed to load input: %v", err)
	}

	wQ, err := testutil.LoadTensor(filepath.Join(testdata, "mha_w_q.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load WQuery: %v", err)
	}

	wK, err := testutil.LoadTensor(filepath.Join(testdata, "mha_w_k.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load WKey: %v", err)
	}

	wV, err := testutil.LoadTensor(filepath.Join(testdata, "mha_w_v.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load WValue: %v", err)
	}

	wOut, err := testutil.LoadTensor(filepath.Join(testdata, "mha_w_out.bin"), []int{768, 768})
	if err != nil {
		t.Fatalf("Failed to load OutProj: %v", err)
	}

	expected, err := testutil.LoadTensor(filepath.Join(testdata, "mha_expected.bin"), []int{2, 10, 768})
	if err != nil {
		t.Fatalf("Failed to load expected output: %v", err)
	}

	// Create MHA with loaded weights
	attn := &MultiHeadAttention{
		NumHeads: 12,
		HeadDim:  64,
		DOut:     768,
		DIn:      768,
		Dropout:  0.0,
		QKVBias:  false,
		WQuery:   wQ,
		WKey:     wK,
		WValue:   wV,
		OutProj:  wOut,
	}

	// Create causal mask
	mask := tensor.CreateCausalMask(10)

	// Forward pass (training=false for deterministic testing)
	output, err := attn.Forward(x, mask, false)
	if err != nil {
		t.Fatalf("Forward failed: %v", err)
	}

	// Compare with Python output
	equal, msg := testutil.TensorsEqual(output, expected, 1e-4)
	if !equal {
		t.Errorf("Output doesn't match Python: %s", msg)
		testutil.PrintTensorComparison(output, expected, "MultiHeadAttention", 10)
	}
}

// createCausalMask creates an upper triangular causal mask for attention.
// Shape: (seq_len, seq_len), with 1s in lower triangle and 0s in upper triangle.
func createCausalMask(seqLen int) *tensor.Tensor {
	mask := tensor.NewTensor([]int{seqLen, seqLen})
	for i := 0; i < seqLen; i++ {
		for j := 0; j < seqLen; j++ {
			if j <= i {
				mask.Data[i*seqLen+j] = 1
			}
		}
	}
	return mask
}
